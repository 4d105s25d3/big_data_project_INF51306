{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.1</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>Warning: You are not running the latest version of PixieDust. Current is 1.1.1, Latest is 1.1.2</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div>Please copy and run the following command in a new cell to upgrade: <span style=\"background-color:#ececec;font-family:monospace;padding:0 5px\">!pip install --user --upgrade pixiedust</span></div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>Please restart kernel after upgrading.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully enabled Spark Job Progress Monitor\n"
     ]
    }
   ],
   "source": [
    "import pixiedust\n",
    "pixiedust.enableJobMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has 3678 lines\n",
      "\n",
      "The header of this file is \n",
      "['Main_Key', 'Date', 'Customer_Code', 'House', 'Flock', 'Gene_Line', 'Birds_Begin', 'Hatch_Date', 'Arrive_Date', 'Remove_date', 'Deactivate_Date', 'Veterinarian', 'Hatchery', 'Age', 'Birds_Present', 'Birds_thinned', 'Death', 'Death_Cum', 'Total_Death_Rate', 'Alive_Rate', 'Body_Weight_g', 'Uniformity_Rate', 'Daily_Gaing', 'Avg_Daily_Gaing_Per_Day', 'Feed_Intake_Per_house_kg', 'FCR_Cum', 'Wheat_Per_Bird_Cum', 'Wheat_Per_Bird', 'Wheat_Per_Day', 'Feed_Intake_Per_Bird_Housed_Cum_kg', 'Feed_Intake_Per_Bird_g', 'Wheat_g', 'FCR', 'Water_l', 'Water_Intake_Per_Bird_ml', 'Water_Intake_Per_Bird_Cum_l', 'Water_Feed', 'Water_FeedCum', 'Comment']\n",
      "\n",
      "The header of this file has 39 lines\n",
      "Distribution of length of lines in this file: \n",
      "[(38, 3676), (1, 1)]\n",
      "\n",
      "There are 76 different main keys\n",
      "There are 3673 lines with vaild main key\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extract head and content from file\n",
    "\"\"\"\n",
    "rdd = sc.textFile('../Project/Data/Broilers_Data.txt')\n",
    "\n",
    "lineSpliter = rdd.map(lambda x: x.split('\\t'))\n",
    "print(\"The file has {0} lines\\n\".format(lineSpliter.count()))\n",
    "\n",
    "oldHeader = ['MainKey']\n",
    "\n",
    "header = ['Main_Key', 'Date', 'Customer_Code', 'House', 'Flock', 'Gene_Line', 'Birds_Begin', 'Hatch_Date', 'Arrive_Date',\\\n",
    " 'Remove_date', 'Deactivate_Date', 'Veterinarian', 'Hatchery', 'Age', 'Birds_Present', 'Birds_thinned', 'Death',\\\n",
    " 'Death_Cum', 'Total_Death_Rate', 'Alive_Rate', 'Body_Weight_g', 'Uniformity_Rate', 'Daily_Gaing', 'Avg_Daily_Gaing_Per_Day',\\\n",
    " 'Feed_Intake_Per_house_kg', 'FCR_Cum', 'Wheat_Per_Bird_Cum', 'Wheat_Per_Bird', 'Wheat_Per_Day',\\\n",
    " 'Feed_Intake_Per_Bird_Housed_Cum_kg', 'Feed_Intake_Per_Bird_g', 'Wheat_g', 'FCR', 'Water_l', 'Water_Intake_Per_Bird_ml',\\\n",
    " 'Water_Intake_Per_Bird_Cum_l', 'Water_Feed', 'Water_FeedCum', 'Comment']\n",
    "\n",
    "# Codes in for-loop is useless now\n",
    "for col in lineSpliter.take(1)[0]:\n",
    "    oldHeader.append(col.strip()\n",
    "                     .replace(' ', '')\n",
    "                     .replace('#', '')\n",
    "                     .replace('(', '')\n",
    "                     .replace(')', '')\n",
    "                     .replace('.', '')\n",
    "                     .replace('/', 'Per')\n",
    "                     .replace('%', 'Percentage'))\n",
    "\n",
    "print(\"The header of this file is \\n{0}\\n\".format(header))\n",
    "print(\"The header of this file has {0} lines\".format(len(header)))\n",
    "\n",
    "data = lineSpliter.filter(lambda x: x[0] != \"Date\")\n",
    "#print(\"Data in this file has {0} lines\".format(data.count()))\n",
    "\n",
    "def lengthInspector(rdd):\n",
    "    lenCounter = rdd.map(lambda x: (len(x), 1))\\\n",
    "                 .reduceByKey(lambda x, y: x + y)\n",
    "    lenList = lenCounter.collect()\n",
    "    return lenList\n",
    "\n",
    "lenList = lengthInspector(data)\n",
    "print(\"Distribution of length of lines in this file: \")\n",
    "print(lenList)\n",
    "\n",
    "\"\"\"\n",
    "Clean the empty lines, calculate main key and clean illegal main key\n",
    "\"\"\"\n",
    "def buildMainKey(line):\n",
    "    mainKey = \"Farm \" + line[1].strip() + \" House \" + line[2].strip() + \" Flock \" + line[3].strip()\n",
    "    newline = [mainKey] + line\n",
    "    return newline\n",
    "\n",
    "emptyClener = data.filter(lambda x: len(x) >= 38)\n",
    "#print(\"Not empty data has {0} lines\".format(emptyClener.count()))\n",
    "\n",
    "mainKeyCounter = emptyClener.map(lambda x: buildMainKey(x))\\\n",
    "                            .map(lambda x: (x[0], 1))\\\n",
    "                            .reduceByKey(lambda x, y: x + y)\n",
    "print(\"\\nThere are {0} different main keys\".format(mainKeyCounter.count()))\n",
    "\n",
    "errorMainKeyCleaner = emptyClener.filter(lambda x: False if x[3] == \"\" else True)\n",
    "print(\"There are {0} lines with vaild main key\".format(errorMainKeyCleaner.count()))\n",
    "\n",
    "cleanData = errorMainKeyCleaner.map(lambda x: buildMainKey(x))\n",
    "cleanMainKeyCounter = cleanData.map(lambda x: (x[0], 1))\\\n",
    "                               .reduceByKey(lambda x, y: x + y)\n",
    "mainKeysWithCounts = cleanMainKeyCounter.collect()\n",
    "mainKeys = cleanMainKeyCounter.map(lambda x: x[0]).collect()\n",
    "#print(\"The main keys are:\")\n",
    "for mainkey in mainKeys:\n",
    "    pass#print(mainkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Farm A House 1 Flock 2016-07', 2: 'Farm A House 1 Flock 2016-10', 3: 'Farm B House 3 Flock 2016-11', 4: 'Farm B House 1 Flock 2016-11', 5: 'Farm B House 3 Flock 2016-12', 6: 'Farm B House 2 Flock 2017-01', 7: 'Farm B House 2 Flock 17-3', 8: 'Farm B House 1 Flock 17-3', 9: 'Farm C House 2 Flock Koppel 11 stal 2', 10: 'Farm D House 1 Flock 2016-07', 11: 'Farm D House 3 Flock 2016-07', 12: 'Farm D House 2 Flock 2016-09', 13: 'Farm D House 2 Flock 2016-11', 14: 'Farm D House 3 Flock 2016-12', 15: 'Farm D House 1 Flock 2016-12', 16: 'Farm D House 2 Flock 2017-02', 17: 'Farm D House 3 Flock 2017-04', 18: 'Farm D House 2 Flock 2017-06', 19: 'Farm D House 3 Flock 2017-06', 20: 'Farm D House 1 Flock 2017-10', 21: 'Farm D House 3 Flock 2017-10', 22: 'Farm E House 5 Flock 2016-09', 23: 'Farm E House 6 Flock 2016-11', 24: 'Farm E House 8 Flock 2016-11', 25: 'Farm E House 7 Flock 2017-01', 26: 'Farm E House 8 Flock 2017-01', 27: 'Farm E House 8 Flock 2017-04', 28: 'Farm E House 7 Flock 2017-04', 29: 'Farm E House 7 Flock 2017-07', 30: 'Farm E House 8 Flock 2017-07', 31: 'Farm E House 6 Flock 2017-07', 32: 'Farm E House 5 Flock 2017-07', 33: 'Farm E House 8 Flock 14-9-2017', 34: 'Farm E House 7 Flock 14-9-2017', 35: 'Farm A House 1 Flock 2016-12', 36: 'Farm A House 1 Flock 2017-02', 37: 'Farm B House 2 Flock 2016-11', 38: 'Farm B House 2 Flock 2016-12', 39: 'Farm B House 1 Flock 2016-12', 40: 'Farm B House 1 Flock 2017-01', 41: 'Farm B House 3 Flock 2017-01', 42: 'Farm B House 3 Flock 17-03-2017', 43: 'Farm C House 1 Flock 2016-09', 44: 'Farm C House 2 Flock 2016-09', 45: 'Farm C House 1 Flock Koppel 11', 46: 'Farm C House 2 Flock Beter Leven Stal 2 ronde 12', 47: 'Farm C House 1 Flock Beter Leven Stal 1 ronde 12', 48: 'Farm D House 2 Flock 2016-07', 49: 'Farm D House 3 Flock 2016-09', 50: 'Farm D House 1 Flock 2016-09', 51: 'Farm D House 1 Flock 2016-11', 52: 'Farm D House 3 Flock 2016-11', 53: 'Farm D House 2 Flock 2016-12', 54: 'Farm D House 3 Flock 2017-02', 55: 'Farm D House 1 Flock 2017-02', 56: 'Farm D House 1 Flock 2017-04', 57: 'Farm D House 2 Flock 2017-04', 58: 'Farm D House 1 Flock 2017-06', 59: 'Farm D House 1 Flock 2017-08', 60: 'Farm D House 3 Flock 2017-08', 61: 'Farm D House 2 Flock 2017-08', 62: 'Farm D House 2 Flock 2017-10', 63: 'Farm E House 6 Flock 2016-09', 64: 'Farm E House 7 Flock 2016-09', 65: 'Farm E House 8 Flock 2016-09', 66: 'Farm E House 7 Flock 2016-11', 67: 'Farm E House 5 Flock 2016-11', 68: 'Farm E House 5 Flock 2017-04', 69: 'Farm E House 6 Flock 2017-01', 70: 'Farm E House 6 Flock 2017-04', 71: 'Farm E House 5 Flock 2017-01', 72: 'Farm E House 6 Flock 18-9-2017', 73: 'Farm E House 5 Flock 14-9-2017'}\n",
      "{0: 'Main_Key', 1: 'Date', 2: 'Customer_Code', 3: 'House', 4: 'Flock', 5: 'Gene_Line', 6: 'Birds_Begin', 7: 'Hatch_Date', 8: 'Arrive_Date', 9: 'Remove_date', 10: 'Deactivate_Date', 11: 'Veterinarian', 12: 'Hatchery', 13: 'Age', 14: 'Birds_Present', 15: 'Birds_thinned', 16: 'Death', 17: 'Death_Cum', 18: 'Total_Death_Rate', 19: 'Alive_Rate', 20: 'Body_Weight_g', 21: 'Uniformity_Rate', 22: 'Daily_Gaing', 23: 'Avg_Daily_Gaing_Per_Day', 24: 'Feed_Intake_Per_house_kg', 25: 'FCR_Cum', 26: 'Wheat_Per_Bird_Cum', 27: 'Wheat_Per_Bird', 28: 'Wheat_Per_Day', 29: 'Feed_Intake_Per_Bird_Housed_Cum_kg', 30: 'Feed_Intake_Per_Bird_g', 31: 'Wheat_g', 32: 'FCR', 33: 'Water_l', 34: 'Water_Intake_Per_Bird_ml', 35: 'Water_Intake_Per_Bird_Cum_l', 36: 'Water_Feed', 37: 'Water_FeedCum', 38: 'Comment'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "mainKeyArror = range(1, 74)\n",
    "numMainKey = dict(zip(mainKeyArror, mainKeys))\n",
    "print(numMainKey)\n",
    "\n",
    "dataIndexArror = range(39)\n",
    "numDataIndex = dict(zip(dataIndexArror, header))\n",
    "print(numDataIndex)\n",
    "\n",
    "def dataTaker(mainKey, dataIndex):\n",
    "    \"\"\"\n",
    "    Return a list of data under given main key and data idx\n",
    "    \"\"\"\n",
    "    if type(mainKey) is int:\n",
    "        mainKey = numMainKey[mainKey]\n",
    "    if type(dataIndex) is str:\n",
    "        for key in numDataIndex:\n",
    "            if dataIndex == numDataIndex[key]: \n",
    "                dataIndex = key\n",
    "                break\n",
    "    \n",
    "    result = cleanData.filter(lambda line: line[0] == mainKey)\\\n",
    "                      .map(lambda line: line[dataIndex])\\\n",
    "                      .collect() # dataIndex should not minus 1 because main key is line[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "def reduceDot(linex, liney):\n",
    "    new_list = []\n",
    "    for i in range(len(linex)):\n",
    "        new_list.append(linex[i] or liney[i])\n",
    "    return new_list\n",
    "\n",
    "checkAllDot = cleanData.map(lambda x: [('.' in i) for i in x])\\\n",
    "                       .reduce(lambda x, y: reduceDot(x, y))\n",
    "print(checkAllDot)\n",
    "\n",
    "def numberCleaner(line):\n",
    "    \"\"\"\n",
    "    This function needs to be improved\n",
    "    \"\"\"\n",
    "    resultList = []\n",
    "    item = None\n",
    "    for idx in range(len(line)):\n",
    "        if idx <= 12 or idx == 38:\n",
    "            if idx == 6: \n",
    "                if not line[idx]: item = -1\n",
    "                else:\n",
    "                    item = line[idx].strip().replace(',', '').replace(' ', '')\n",
    "                    item = int(item)\n",
    "            else: \n",
    "                if not line[idx]: item = \"\"\n",
    "                else: item = line[idx]\n",
    "        elif checkAllDot[idx]:\n",
    "            if not line[idx]: item = -0.1\n",
    "            else:\n",
    "                item = line[idx].strip().replace(',', '').replace(' ', '')\n",
    "                item = float(item)\n",
    "        else: \n",
    "            if not line[idx]: item = -1\n",
    "            else:\n",
    "                item = line[idx].strip().replace(',', '').replace(' ', '').replace('\"', '')\n",
    "                item = int(item)\n",
    "        resultList.append(item)\n",
    "    return resultList\n",
    "\n",
    "numCleanData = cleanData.map(numberCleaner)\n",
    "\n",
    "def transferNone(line):\n",
    "    new_line = []\n",
    "    for item in line:\n",
    "        if type(item) != str:\n",
    "            if item < 0: new_line.append(None)\n",
    "            else: new_line.append(item)\n",
    "        else:\n",
    "            if not item: new_line.append(None)\n",
    "            else:new_line.append(item)\n",
    "    return new_line\n",
    "\n",
    "broilers = numCleanData.map(transferNone)\n",
    "\n",
    "broilersDF = sqlContext.createDataFrame(numCleanData, header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Main_Key', 'Date', 'Customer_Code', 'House', 'Flock', 'Gene_Line', 'Birds_Begin', 'Hatch_Date', 'Arrive_Date', 'Remove_date', 'Deactivate_Date', 'Veterinarian', 'Hatchery', 'Age', 'Birds_Present', 'Birds_thinned', 'Death', 'Death_Cum', 'Total_Death_Rate', 'Alive_Rate', 'Body_Weight_g', 'Uniformity_Rate', 'Daily_Gaing', 'Avg_Daily_Gaing_Per_Day', 'Feed_Intake_Per_house_kg', 'FCR_Cum', 'Wheat_Per_Bird_Cum', 'Wheat_Per_Bird', 'Wheat_Per_Day', 'Feed_Intake_Per_Bird_Housed_Cum_kg', 'Feed_Intake_Per_Bird_g', 'Wheat_g', 'FCR', 'Water_l', 'Water_Intake_Per_Bird_ml', 'Water_Intake_Per_Bird_Cum_l', 'Water_Feed', 'Water_FeedCum', 'Comment']\n",
      "['Farm A House 1 Flock 2016-07', 'Farm A House 1 Flock 2016-10', 'Farm B House 3 Flock 2016-11', 'Farm B House 1 Flock 2016-11', 'Farm B House 3 Flock 2016-12', 'Farm B House 2 Flock 2017-01', 'Farm B House 2 Flock 17-3', 'Farm B House 1 Flock 17-3', 'Farm C House 2 Flock Koppel 11 stal 2', 'Farm D House 1 Flock 2016-07', 'Farm D House 3 Flock 2016-07', 'Farm D House 2 Flock 2016-09', 'Farm D House 2 Flock 2016-11', 'Farm D House 3 Flock 2016-12', 'Farm D House 1 Flock 2016-12', 'Farm D House 2 Flock 2017-02', 'Farm D House 3 Flock 2017-04', 'Farm D House 2 Flock 2017-06', 'Farm D House 3 Flock 2017-06', 'Farm D House 1 Flock 2017-10', 'Farm D House 3 Flock 2017-10', 'Farm E House 5 Flock 2016-09', 'Farm E House 6 Flock 2016-11', 'Farm E House 8 Flock 2016-11', 'Farm E House 7 Flock 2017-01', 'Farm E House 8 Flock 2017-01', 'Farm E House 8 Flock 2017-04', 'Farm E House 7 Flock 2017-04', 'Farm E House 7 Flock 2017-07', 'Farm E House 8 Flock 2017-07', 'Farm E House 6 Flock 2017-07', 'Farm E House 5 Flock 2017-07', 'Farm E House 8 Flock 14-9-2017', 'Farm E House 7 Flock 14-9-2017', 'Farm A House 1 Flock 2016-12', 'Farm A House 1 Flock 2017-02', 'Farm B House 2 Flock 2016-11', 'Farm B House 2 Flock 2016-12', 'Farm B House 1 Flock 2016-12', 'Farm B House 1 Flock 2017-01', 'Farm B House 3 Flock 2017-01', 'Farm B House 3 Flock 17-03-2017', 'Farm C House 1 Flock 2016-09', 'Farm C House 2 Flock 2016-09', 'Farm C House 1 Flock Koppel 11', 'Farm C House 2 Flock Beter Leven Stal 2 ronde 12', 'Farm C House 1 Flock Beter Leven Stal 1 ronde 12', 'Farm D House 2 Flock 2016-07', 'Farm D House 3 Flock 2016-09', 'Farm D House 1 Flock 2016-09', 'Farm D House 1 Flock 2016-11', 'Farm D House 3 Flock 2016-11', 'Farm D House 2 Flock 2016-12', 'Farm D House 3 Flock 2017-02', 'Farm D House 1 Flock 2017-02', 'Farm D House 1 Flock 2017-04', 'Farm D House 2 Flock 2017-04', 'Farm D House 1 Flock 2017-06', 'Farm D House 1 Flock 2017-08', 'Farm D House 3 Flock 2017-08', 'Farm D House 2 Flock 2017-08', 'Farm D House 2 Flock 2017-10', 'Farm E House 6 Flock 2016-09', 'Farm E House 7 Flock 2016-09', 'Farm E House 8 Flock 2016-09', 'Farm E House 7 Flock 2016-11', 'Farm E House 5 Flock 2016-11', 'Farm E House 5 Flock 2017-04', 'Farm E House 6 Flock 2017-01', 'Farm E House 6 Flock 2017-04', 'Farm E House 5 Flock 2017-01', 'Farm E House 6 Flock 18-9-2017', 'Farm E House 5 Flock 14-9-2017']\n",
      "3494\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def selectArea(Main_Keys, inCols):\n",
    "    selectRows = udf(lambda x: x in Main_Keys, BooleanType())\n",
    "    area = broilersDF.filter(selectRows(broilersDF.Main_Key)).select(inCols)\n",
    "    return area\n",
    "\n",
    "print(header)\n",
    "print(mainKeys)\n",
    "\n",
    "test = ['Age', 'Death']\n",
    "samples = [[] for i in range(len(mainKeys))]\n",
    "for i in range(len(mainKeys)):\n",
    "    samples[i] = selectArea(mainKeys[i], test)\n",
    "\n",
    "sample_train = selectArea(mainKeys[2:], test)\n",
    "sample_test = selectArea(mainKeys[1:3], test)\n",
    "\n",
    "print(sample_train.count())\n",
    "print(sample_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is for preparing datasets used in regression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attributes of the model are: ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__metaclass__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_clear', '_copyValues', '_copy_params', '_defaultParamMap', '_dummy', '_from_java', '_paramMap', '_params', '_randomUID', '_resetUid', '_resolveParam', '_set', '_setDefault', '_shouldOwn', '_to_java', '_transform', 'copy', 'explainParam', 'explainParams', 'extractParamMap', 'getOrDefault', 'getParam', 'hasDefault', 'hasParam', 'isDefined', 'isSet', 'load', 'params', 'read', 'save', 'stages', 'transform', 'uid', 'write']\n",
      "method list: ['__class__', '__delattr__', '__dir__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__metaclass__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_clear', '_copyValues', '_copy_params', '_dummy', '_from_java', '_randomUID', '_resetUid', '_resolveParam', '_set', '_setDefault', '_shouldOwn', '_to_java', '_transform', 'copy', 'explainParam', 'explainParams', 'extractParamMap', 'getOrDefault', 'getParam', 'hasDefault', 'hasParam', 'isDefined', 'isSet', 'load', 'read', 'save', 'transform', 'write']\n",
      "[VectorAssembler_4d1bbf8f97627936759c, LinearRegression_4dc5b27433f5df6b3c15]\n",
      "[('Age', -0.21001609233115143)]\n",
      "19.73969526316345\n",
      "New Root Mean Squared Error: 19.91\n",
      "+---+-----+--------+------------------+\n",
      "|Age|Death|features|         Predicted|\n",
      "+---+-----+--------+------------------+\n",
      "|  0|    0|   [0.0]| 19.73969526316345|\n",
      "|  1|    0|   [1.0]|  19.5296791708323|\n",
      "|  2|   52|   [2.0]|19.319663078501147|\n",
      "|  3|   28|   [3.0]|    19.10964698617|\n",
      "|  4|   81|   [4.0]|18.899630893838847|\n",
      "|  5|   29|   [5.0]|18.689614801507695|\n",
      "|  6|   20|   [6.0]|18.479598709176543|\n",
      "|  7|   19|   [7.0]| 18.26958261684539|\n",
      "|  8|    8|   [8.0]| 18.05956652451424|\n",
      "|  9|   15|   [9.0]| 17.84955043218309|\n",
      "| 10|   13|  [10.0]|17.639534339851938|\n",
      "| 11|   15|  [11.0]|17.429518247520786|\n",
      "| 12|   24|  [12.0]|17.219502155189634|\n",
      "| 13|   15|  [13.0]|17.009486062858482|\n",
      "| 14|   13|  [14.0]| 16.79946997052733|\n",
      "| 15|    7|  [15.0]| 16.58945387819618|\n",
      "| 16|   24|  [16.0]| 16.37943778586503|\n",
      "| 17|   18|  [17.0]|16.169421693533877|\n",
      "| 18|   14|  [18.0]|15.959405601202725|\n",
      "| 19|   16|  [19.0]|15.749389508871573|\n",
      "+---+-----+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, LinearRegression, LinearRegressionModel, RandomForestRegressor\n",
    "\n",
    "def Do_Machine_Learning(trainingSet, testSet, xCols, yValues, regressor = LinearRegression(),\n",
    "                         paramGrid = [], evalMetric = \"rmse\", seed = None):\n",
    "    \"\"\"\n",
    "    Return (<object> model, <float> error_estimate, <dataframe> result)\n",
    "    \n",
    "    trainingSet: dataframe, used to train model\n",
    "    testSet: dataframe, used to feed the model and get the result(and error estimate)\n",
    "    xCols: list of strings, names of columns which used as inputs\n",
    "    yValues: string, name of column that contains dependent values \n",
    "    regressor: Regression object, by default = LinearRegression()\n",
    "    paramGrid: list built byParamGridBuilder, by default = empty list\n",
    "    evalMetric: string, name of matrix used for evaluation, by default = \"rmse\"\n",
    "    seed: int or None, seed for random number generator, if == None will use random numbers\n",
    "    \n",
    "    !!! seed is useless at that time !!!\n",
    "    \"\"\"\n",
    "    # push estimator into pipeline\n",
    "    vec = VectorAssembler(inputCols = xCols, outputCol = \"features\")\n",
    "    regPipeline = Pipeline()\n",
    "    regPipeline.setStages([vec, regressor])   \n",
    "    # build evaluator\n",
    "    regEval = RegressionEvaluator(predictionCol = \"Predicted\", labelCol = yValues, metricName = evalMetric)\n",
    "    # combine estimator and evaluator to a cross validator\n",
    "    crossval = CrossValidator(estimator = regPipeline, evaluator = regEval, numFolds = 3)\n",
    "    # set parameters grid\n",
    "    crossval.setEstimatorParamMaps(paramGrid)\n",
    "    # trainning\n",
    "    regModel = crossval.fit(trainingSet).bestModel\n",
    "    # predicting\n",
    "    predictions = regModel.transform(testSet)\n",
    "    # get evaluating result\n",
    "    evaluation = regEval.evaluate(predictions)\n",
    "    \n",
    "    return regModel, evaluation, predictions\n",
    "\n",
    "# build regressor\n",
    "lr1 = LinearRegression()\n",
    "lr1.setPredictionCol(\"Predicted\")\\\n",
    "   .setLabelCol(\"Death\")\n",
    "\n",
    "# build parameter grid\n",
    "regParam = [x / 100.0 for x in range(1, 10)]\n",
    "pg1 = (ParamGridBuilder()\n",
    "             .addGrid(lr1.regParam, regParam)\n",
    "             .build())\n",
    "\n",
    "print(type(lr1))\n",
    "print(type(pg1))\n",
    "\n",
    "# run ML\n",
    "model, result, predictionDF = Do_Machine_Learning(sample_train, sample_test, [\"Age\"], \"Death\", lr1, pg1)\n",
    "\n",
    "# print attributions of model\n",
    "\"\"\"\n",
    "print(\"attributes of the model are: {}\".format(dir(model)))\n",
    "print(\"method list: {}\".format([method for method in dir(model) if callable(getattr(model, method))]))\n",
    "print(model.stages)\n",
    "\"\"\"\n",
    "\n",
    "# Print coefficients and intercept\n",
    "weights = model.stages[1].coefficients\n",
    "print(list(zip([\"Age\"], weights)))\n",
    "print(model.stages[1].intercept)\n",
    "\n",
    "# print error and result\n",
    "print(\"New Root Mean Squared Error: {0:2.2f}\\n\".format(result))\n",
    "print(predictionDF.show())\n",
    "\n",
    "# print the model\n",
    "print(model.stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.regression.LinearRegression'>\n",
      "[{Param(parent='LinearRegression_4dc5b27433f5df6b3c15', name='regParam', doc='regularization parameter (>= 0).'): 0.01}, {Param(parent='LinearRegression_4dc5b27433f5df6b3c15', name='regParam', doc='regularization parameter (>= 0).'): 0.02}, {Param(parent='LinearRegression_4dc5b27433f5df6b3c15', name='regParam', doc='regularization parameter (>= 0).'): 0.03}, {Param(parent='LinearRegression_4dc5b27433f5df6b3c15', name='regParam', doc='regularization parameter (>= 0).'): 0.04}, {Param(parent='LinearRegression_4dc5b27433f5df6b3c15', name='regParam', doc='regularization parameter (>= 0).'): 0.05}, {Param(parent='LinearRegression_4dc5b27433f5df6b3c15', name='regParam', doc='regularization parameter (>= 0).'): 0.06}, {Param(parent='LinearRegression_4dc5b27433f5df6b3c15', name='regParam', doc='regularization parameter (>= 0).'): 0.07}, {Param(parent='LinearRegression_4dc5b27433f5df6b3c15', name='regParam', doc='regularization parameter (>= 0).'): 0.08}, {Param(parent='LinearRegression_4dc5b27433f5df6b3c15', name='regParam', doc='regularization parameter (>= 0).'): 0.09}]\n"
     ]
    }
   ],
   "source": [
    "print(type(lr1))\n",
    "print(pg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python with Pixiedust (Spark 2.2)",
   "language": "python",
   "name": "pythonwithpixiedustspark22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
