{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.1</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>Warning: You are not running the latest version of PixieDust. Current is 1.1.1, Latest is 1.1.3</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div>Please copy and run the following command in a new cell to upgrade: <span style=\"background-color:#ececec;font-family:monospace;padding:0 5px\">!pip install --user --upgrade pixiedust</span></div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>Please restart kernel after upgrading.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully enabled Spark Job Progress Monitor\n"
     ]
    }
   ],
   "source": [
    "import pixiedust\n",
    "pixiedust.enableJobMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has 3677 lines\n",
      "\n",
      "The header of this file is \n",
      "['Main_Key', 'Date', 'Customer_Code', 'House', 'Flock', 'Gene_Line', 'Birds_Begin', 'Hatch_Date', 'Arrive_Date', 'Remove_date', 'Deactivate_Date', 'Veterinarian', 'Hatchery', 'Age', 'Birds_Present', 'Birds_thinned', 'Death', 'Death_Cum', 'Total_Death_Rate', 'Alive_Rate', 'Body_Weight_g', 'Uniformity_Rate', 'Daily_Gaing', 'Avg_Daily_Gaing_Per_Day', 'Feed_Intake_Per_house_kg', 'FCR_Cum', 'Wheat_Per_Bird_Cum', 'Wheat_Per_Bird', 'Wheat_Per_Day', 'Feed_Intake_Per_Bird_Housed_Cum_kg', 'Feed_Intake_Per_Bird_g', 'Wheat_g', 'FCR', 'Water_l', 'Water_Intake_Per_Bird_ml', 'Water_Intake_Per_Bird_Cum_l', 'Water_Feed', 'Water_FeedCum', 'Comment']\n",
      "\n",
      "The header of this file has 39 lines\n",
      "Distribution of length of lines in this file: \n",
      "[(38, 3676)]\n",
      "\n",
      "There are 76 different main keys\n",
      "There are 3673 lines with vaild main key\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extract head and content from file\n",
    "\"\"\"\n",
    "rdd = sc.textFile('../Project/Data/Broilers_Data.txt')\n",
    "\n",
    "lineSpliter = rdd.map(lambda x: x.split('\\t'))\n",
    "print(\"The file has {0} lines\\n\".format(lineSpliter.count()))\n",
    "\n",
    "oldHeader = ['MainKey']\n",
    "\n",
    "header = ['Main_Key', 'Date', 'Customer_Code', 'House', 'Flock', 'Gene_Line', 'Birds_Begin', 'Hatch_Date', 'Arrive_Date',\\\n",
    " 'Remove_date', 'Deactivate_Date', 'Veterinarian', 'Hatchery', 'Age', 'Birds_Present', 'Birds_thinned', 'Death',\\\n",
    " 'Death_Cum', 'Total_Death_Rate', 'Alive_Rate', 'Body_Weight_g', 'Uniformity_Rate', 'Daily_Gaing', 'Avg_Daily_Gaing_Per_Day',\\\n",
    " 'Feed_Intake_Per_house_kg', 'FCR_Cum', 'Wheat_Per_Bird_Cum', 'Wheat_Per_Bird', 'Wheat_Per_Day',\\\n",
    " 'Feed_Intake_Per_Bird_Housed_Cum_kg', 'Feed_Intake_Per_Bird_g', 'Wheat_g', 'FCR', 'Water_l', 'Water_Intake_Per_Bird_ml',\\\n",
    " 'Water_Intake_Per_Bird_Cum_l', 'Water_Feed', 'Water_FeedCum', 'Comment']\n",
    "\n",
    "# Codes in for-loop is useless now\n",
    "for col in lineSpliter.take(1)[0]:\n",
    "    oldHeader.append(col.strip()\n",
    "                     .replace(' ', '')\n",
    "                     .replace('#', '')\n",
    "                     .replace('(', '')\n",
    "                     .replace(')', '')\n",
    "                     .replace('.', '')\n",
    "                     .replace('/', 'Per')\n",
    "                     .replace('%', 'Percentage'))\n",
    "\n",
    "print(\"The header of this file is \\n{0}\\n\".format(header))\n",
    "print(\"The header of this file has {0} lines\".format(len(header)))\n",
    "\n",
    "data = lineSpliter.filter(lambda x: x[0] != \"Date\")\n",
    "#print(\"Data in this file has {0} lines\".format(data.count()))\n",
    "\n",
    "def lengthInspector(rdd):\n",
    "    lenCounter = rdd.map(lambda x: (len(x), 1))\\\n",
    "                 .reduceByKey(lambda x, y: x + y)\n",
    "    lenList = lenCounter.collect()\n",
    "    return lenList\n",
    "\n",
    "lenList = lengthInspector(data)\n",
    "print(\"Distribution of length of lines in this file: \")\n",
    "print(lenList)\n",
    "\n",
    "\"\"\"\n",
    "Clean the empty lines, calculate main key and clean illegal main key\n",
    "\"\"\"\n",
    "def buildMainKey(line):\n",
    "    mainKey = \"Farm \" + line[1].strip() + \" House \" + line[2].strip() + \" Flock \" + line[3].strip()\n",
    "    newline = [mainKey] + line\n",
    "    return newline\n",
    "\n",
    "emptyClener = data.filter(lambda x: len(x) >= 38)\n",
    "#print(\"Not empty data has {0} lines\".format(emptyClener.count()))\n",
    "\n",
    "mainKeyCounter = emptyClener.map(lambda x: buildMainKey(x))\\\n",
    "                            .map(lambda x: (x[0], 1))\\\n",
    "                            .reduceByKey(lambda x, y: x + y)\n",
    "print(\"\\nThere are {0} different main keys\".format(mainKeyCounter.count()))\n",
    "\n",
    "errorMainKeyCleaner = emptyClener.filter(lambda x: False if x[3] == \"\" else True)\n",
    "print(\"There are {0} lines with vaild main key\".format(errorMainKeyCleaner.count()))\n",
    "\n",
    "cleanData = errorMainKeyCleaner.map(lambda x: buildMainKey(x))\n",
    "cleanMainKeyCounter = cleanData.map(lambda x: (x[0], 1))\\\n",
    "                               .reduceByKey(lambda x, y: x + y)\n",
    "mainKeysWithCounts = cleanMainKeyCounter.collect()\n",
    "mainKeys = cleanMainKeyCounter.map(lambda x: x[0]).collect()\n",
    "#print(\"The main keys are:\")\n",
    "for mainkey in mainKeys:\n",
    "    pass#print(mainkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Farm B House 3 Flock Nov-16', 2: 'Farm B House 2 Flock Nov-16', 3: 'Farm B House 1 Flock Nov-16', 4: 'Farm B House 2 Flock Jan-17', 5: 'Farm B House 1 Flock Jan-17', 6: 'Farm C House 2 Flock Sep-16', 7: 'Farm C House 2 Flock Koppel 11 stal 2', 8: 'Farm D House 1 Flock Jul-16', 9: 'Farm D House 2 Flock Jul-16', 10: 'Farm D House 3 Flock Sep-16', 11: 'Farm D House 1 Flock Nov-16', 12: 'Farm D House 3 Flock Nov-16', 13: 'Farm D House 2 Flock Nov-16', 14: 'Farm D House 2 Flock Dec-16', 15: 'Farm D House 1 Flock Dec-16', 16: 'Farm D House 1 Flock Apr-17', 17: 'Farm D House 2 Flock Apr-17', 18: 'Farm D House 2 Flock Jun-17', 19: 'Farm D House 1 Flock Jun-17', 20: 'Farm D House 3 Flock Jun-17', 21: 'Farm D House 3 Flock Aug-17', 22: 'Farm D House 2 Flock Aug-17', 23: 'Farm D House 2 Flock Oct-17', 24: 'Farm D House 1 Flock Oct-17', 25: 'Farm D House 3 Flock Oct-17', 26: 'Farm E House 7 Flock Sep-16', 27: 'Farm E House 5 Flock Sep-16', 28: 'Farm E House 6 Flock Nov-16', 29: 'Farm E House 8 Flock Nov-16', 30: 'Farm E House 5 Flock Nov-16', 31: 'Farm E House 7 Flock Jan-17', 32: 'Farm E House 8 Flock Jan-17', 33: 'Farm E House 5 Flock Apr-17', 34: 'Farm E House 6 Flock Jan-17', 35: 'Farm E House 6 Flock Apr-17', 36: 'Farm E House 7 Flock Apr-17', 37: 'Farm E House 5 Flock Jan-17', 38: 'Farm E House 6 Flock Jul-17', 39: 'Farm E House 8 Flock 14-9-2017', 40: 'Farm E House 7 Flock 14-9-2017', 41: 'Farm A House 1 Flock Jul-16', 42: 'Farm A House 1 Flock Oct-16', 43: 'Farm A House 1 Flock Dec-16', 44: 'Farm A House 1 Flock Feb-17', 45: 'Farm B House 2 Flock Dec-16', 46: 'Farm B House 1 Flock Dec-16', 47: 'Farm B House 3 Flock Dec-16', 48: 'Farm B House 3 Flock Jan-17', 49: 'Farm B House 2 Flock 3��17��', 50: 'Farm B House 3 Flock 17-03-2017', 51: 'Farm B House 1 Flock 3��17��', 52: 'Farm C House 1 Flock Sep-16', 53: 'Farm C House 1 Flock Koppel 11', 54: 'Farm C House 2 Flock Beter Leven Stal 2 ronde 12', 55: 'Farm C House 1 Flock Beter Leven Stal 1 ronde 12', 56: 'Farm D House 3 Flock Jul-16', 57: 'Farm D House 2 Flock Sep-16', 58: 'Farm D House 1 Flock Sep-16', 59: 'Farm D House 3 Flock Dec-16', 60: 'Farm D House 3 Flock Feb-17', 61: 'Farm D House 1 Flock Feb-17', 62: 'Farm D House 2 Flock Feb-17', 63: 'Farm D House 3 Flock Apr-17', 64: 'Farm D House 1 Flock Aug-17', 65: 'Farm E House 6 Flock Sep-16', 66: 'Farm E House 8 Flock Sep-16', 67: 'Farm E House 7 Flock Nov-16', 68: 'Farm E House 8 Flock Apr-17', 69: 'Farm E House 7 Flock Jul-17', 70: 'Farm E House 8 Flock Jul-17', 71: 'Farm E House 5 Flock Jul-17', 72: 'Farm E House 6 Flock 18-9-2017', 73: 'Farm E House 5 Flock 14-9-2017'}\n",
      "{0: 'Main_Key', 1: 'Date', 2: 'Customer_Code', 3: 'House', 4: 'Flock', 5: 'Gene_Line', 6: 'Birds_Begin', 7: 'Hatch_Date', 8: 'Arrive_Date', 9: 'Remove_date', 10: 'Deactivate_Date', 11: 'Veterinarian', 12: 'Hatchery', 13: 'Age', 14: 'Birds_Present', 15: 'Birds_thinned', 16: 'Death', 17: 'Death_Cum', 18: 'Total_Death_Rate', 19: 'Alive_Rate', 20: 'Body_Weight_g', 21: 'Uniformity_Rate', 22: 'Daily_Gaing', 23: 'Avg_Daily_Gaing_Per_Day', 24: 'Feed_Intake_Per_house_kg', 25: 'FCR_Cum', 26: 'Wheat_Per_Bird_Cum', 27: 'Wheat_Per_Bird', 28: 'Wheat_Per_Day', 29: 'Feed_Intake_Per_Bird_Housed_Cum_kg', 30: 'Feed_Intake_Per_Bird_g', 31: 'Wheat_g', 32: 'FCR', 33: 'Water_l', 34: 'Water_Intake_Per_Bird_ml', 35: 'Water_Intake_Per_Bird_Cum_l', 36: 'Water_Feed', 37: 'Water_FeedCum', 38: 'Comment'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pixiedust/utils/sparkJobProgressMonitor.py\", line 47, in startSparkJobProgressMonitor\n",
      "    progressMonitor = SparkJobProgressMonitor()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pixiedust/utils/sparkJobProgressMonitor.py\", line 174, in __init__\n",
      "    self.addSparkListener()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pixiedust/utils/sparkJobProgressMonitor.py\", line 203, in addSparkListener\n",
      "    _env.getTemplate(\"sparkJobProgressMonitor/addSparkListener.scala\").render()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2131, in run_cell_magic\n",
      "    result = fn(magic_arg_s, cell)\n",
      "  File \"<decorator-gen-126>\", line 2, in scala\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pixiedust/utils/scalaBridge.py\", line 193, in scala\n",
      "    runnerObject.callMethod(\"set\" + key[0].upper() + key[1:], val[\"initValue\"])\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/pixiedust/utils/javaBridge.py\", line 154, in callMethod\n",
      "    raise ValueError(\"Method {0} that matches the given arguments not found\".format(methodName) )\n",
      "ValueError: Method setRdd that matches the given arguments not found\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "mainKeyArror = range(1, 74)\n",
    "numMainKey = dict(zip(mainKeyArror, mainKeys))\n",
    "print(numMainKey)\n",
    "\n",
    "dataIndexArror = range(39)\n",
    "numDataIndex = dict(zip(dataIndexArror, header))\n",
    "print(numDataIndex)\n",
    "\n",
    "def dataTaker(mainKey, dataIndex):\n",
    "    \"\"\"\n",
    "    Return a list of data under given main key and data idx\n",
    "    \"\"\"\n",
    "    if type(mainKey) is int:\n",
    "        mainKey = numMainKey[mainKey]\n",
    "    if type(dataIndex) is str:\n",
    "        for key in numDataIndex:\n",
    "            if dataIndex == numDataIndex[key]: \n",
    "                dataIndex = key\n",
    "                break\n",
    "    \n",
    "    result = cleanData.filter(lambda line: line[0] == mainKey)\\\n",
    "                      .map(lambda line: line[dataIndex])\\\n",
    "                      .collect() # dataIndex should not minus 1 because main key is line[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "def reduceDot(linex, liney):\n",
    "    new_list = []\n",
    "    for i in range(len(linex)):\n",
    "        new_list.append(linex[i] or liney[i])\n",
    "    return new_list\n",
    "\n",
    "checkAllDot = cleanData.map(lambda x: [('.' in i) for i in x])\\\n",
    "                       .reduce(lambda x, y: reduceDot(x, y))\n",
    "print(checkAllDot)\n",
    "\n",
    "def numberCleaner(line):\n",
    "    \"\"\"\n",
    "    This function needs to be improved\n",
    "    \"\"\"\n",
    "    resultList = []\n",
    "    item = None\n",
    "    for idx in range(len(line)):\n",
    "        if idx <= 12 or idx == 38:\n",
    "            if idx == 6: \n",
    "                if not line[idx]: item = -1\n",
    "                else:\n",
    "                    item = line[idx].strip().replace(',', '').replace(' ', '')\n",
    "                    item = int(item)\n",
    "            else: \n",
    "                if not line[idx]: item = \"\"\n",
    "                else: item = line[idx]\n",
    "        elif checkAllDot[idx]:\n",
    "            if not line[idx]: item = -0.1\n",
    "            else:\n",
    "                item = line[idx].strip().replace(',', '').replace(' ', '')\n",
    "                item = float(item)\n",
    "        else: \n",
    "            if not line[idx]: item = -1\n",
    "            else:\n",
    "                item = line[idx].strip().replace(',', '').replace(' ', '').replace('\"', '')\n",
    "                item = int(item)\n",
    "        resultList.append(item)\n",
    "    return resultList\n",
    "\n",
    "numCleanData = cleanData.map(numberCleaner)\n",
    "\n",
    "def transferNone(line):\n",
    "    new_line = []\n",
    "    for item in line:\n",
    "        if type(item) != str:\n",
    "            if item < 0: new_line.append(None)\n",
    "            else: new_line.append(item)\n",
    "        else:\n",
    "            if not item: new_line.append(None)\n",
    "            else:new_line.append(item)\n",
    "    return new_line\n",
    "\n",
    "broilers = numCleanData.map(transferNone)\n",
    "\n",
    "broilersDF = sqlContext.createDataFrame(numCleanData, header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Main_Key', 'Date', 'Customer_Code', 'House', 'Flock', 'Gene_Line', 'Birds_Begin', 'Hatch_Date', 'Arrive_Date', 'Remove_date', 'Deactivate_Date', 'Veterinarian', 'Hatchery', 'Age', 'Birds_Present', 'Birds_thinned', 'Death', 'Death_Cum', 'Total_Death_Rate', 'Alive_Rate', 'Body_Weight_g', 'Uniformity_Rate', 'Daily_Gaing', 'Avg_Daily_Gaing_Per_Day', 'Feed_Intake_Per_house_kg', 'FCR_Cum', 'Wheat_Per_Bird_Cum', 'Wheat_Per_Bird', 'Wheat_Per_Day', 'Feed_Intake_Per_Bird_Housed_Cum_kg', 'Feed_Intake_Per_Bird_g', 'Wheat_g', 'FCR', 'Water_l', 'Water_Intake_Per_Bird_ml', 'Water_Intake_Per_Bird_Cum_l', 'Water_Feed', 'Water_FeedCum', 'Comment']\n",
      "['Farm B House 3 Flock Nov-16', 'Farm B House 2 Flock Nov-16', 'Farm B House 1 Flock Nov-16', 'Farm B House 2 Flock Jan-17', 'Farm B House 1 Flock Jan-17', 'Farm C House 2 Flock Sep-16', 'Farm C House 2 Flock Koppel 11 stal 2', 'Farm D House 1 Flock Jul-16', 'Farm D House 2 Flock Jul-16', 'Farm D House 3 Flock Sep-16', 'Farm D House 1 Flock Nov-16', 'Farm D House 3 Flock Nov-16', 'Farm D House 2 Flock Nov-16', 'Farm D House 2 Flock Dec-16', 'Farm D House 1 Flock Dec-16', 'Farm D House 1 Flock Apr-17', 'Farm D House 2 Flock Apr-17', 'Farm D House 2 Flock Jun-17', 'Farm D House 1 Flock Jun-17', 'Farm D House 3 Flock Jun-17', 'Farm D House 3 Flock Aug-17', 'Farm D House 2 Flock Aug-17', 'Farm D House 2 Flock Oct-17', 'Farm D House 1 Flock Oct-17', 'Farm D House 3 Flock Oct-17', 'Farm E House 7 Flock Sep-16', 'Farm E House 5 Flock Sep-16', 'Farm E House 6 Flock Nov-16', 'Farm E House 8 Flock Nov-16', 'Farm E House 5 Flock Nov-16', 'Farm E House 7 Flock Jan-17', 'Farm E House 8 Flock Jan-17', 'Farm E House 5 Flock Apr-17', 'Farm E House 6 Flock Jan-17', 'Farm E House 6 Flock Apr-17', 'Farm E House 7 Flock Apr-17', 'Farm E House 5 Flock Jan-17', 'Farm E House 6 Flock Jul-17', 'Farm E House 8 Flock 14-9-2017', 'Farm E House 7 Flock 14-9-2017', 'Farm A House 1 Flock Jul-16', 'Farm A House 1 Flock Oct-16', 'Farm A House 1 Flock Dec-16', 'Farm A House 1 Flock Feb-17', 'Farm B House 2 Flock Dec-16', 'Farm B House 1 Flock Dec-16', 'Farm B House 3 Flock Dec-16', 'Farm B House 3 Flock Jan-17', 'Farm B House 2 Flock 3��17��', 'Farm B House 3 Flock 17-03-2017', 'Farm B House 1 Flock 3��17��', 'Farm C House 1 Flock Sep-16', 'Farm C House 1 Flock Koppel 11', 'Farm C House 2 Flock Beter Leven Stal 2 ronde 12', 'Farm C House 1 Flock Beter Leven Stal 1 ronde 12', 'Farm D House 3 Flock Jul-16', 'Farm D House 2 Flock Sep-16', 'Farm D House 1 Flock Sep-16', 'Farm D House 3 Flock Dec-16', 'Farm D House 3 Flock Feb-17', 'Farm D House 1 Flock Feb-17', 'Farm D House 2 Flock Feb-17', 'Farm D House 3 Flock Apr-17', 'Farm D House 1 Flock Aug-17', 'Farm E House 6 Flock Sep-16', 'Farm E House 8 Flock Sep-16', 'Farm E House 7 Flock Nov-16', 'Farm E House 8 Flock Apr-17', 'Farm E House 7 Flock Jul-17', 'Farm E House 8 Flock Jul-17', 'Farm E House 5 Flock Jul-17', 'Farm E House 6 Flock 18-9-2017', 'Farm E House 5 Flock 14-9-2017']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def selectArea(Main_Keys, inCols):\n",
    "    selectRows = udf(lambda x: x in Main_Keys, BooleanType())\n",
    "    area = broilersDF.filter(selectRows(broilersDF.Main_Key)).select(inCols)\n",
    "    return area\n",
    "\n",
    "print(header)\n",
    "print(mainKeys)\n",
    "\n",
    "test = ['Age', 'Death']\n",
    "\n",
    "def selectRDD(inRows, inCols):\n",
    "    \"\"\"\n",
    "    !!!Replace dataTaker!!!\n",
    "    \n",
    "    Return RDD containing given main keys and data idxs\n",
    "    \"\"\"\n",
    "    newRows = []\n",
    "    newCols = []\n",
    "    for row in inRows:\n",
    "        if type(row) is int: newRows.append(numMainKey[row])\n",
    "        else: newRows.append(row)\n",
    "    for col in inCols:\n",
    "        if type(col) is str:\n",
    "            for key in numDataIndex:\n",
    "                if col == numDataIndex[key]: \n",
    "                    newCols.append(key)\n",
    "                    break\n",
    "        else: newCols.append(col)\n",
    "    \n",
    "    newRDD = broilers.filter(lambda line: line[0] in newRows)\\\n",
    "                      .map(lambda line: [line[idx] for idx in newCols])\n",
    "    #result = sqlContext.createDataFrame(newRDD, inCols)\n",
    "    return inCols, newRDD\n",
    "\n",
    "testCols, testRDD = selectRDD(mainKeys[1:3], test)\n",
    "trainCols, trainRDD = selectRDD(mainKeys[2:], test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4], [1, 3], [2, 24], [2, 55], [3, 71], [3, 64], [4, 44], [4, 33], [5, 11], [5, 17], [6, 14], [6, 16], [7, 11], [7, 20], [8, 17], [8, 13], [9, 18], [9, 19], [10, 13], [10, 13]]\n",
      "+---+-----+------+------------------+------------------+------------------+\n",
      "|Age|Death|square|              sqrt|                ln|               exp|\n",
      "+---+-----+------+------------------+------------------+------------------+\n",
      "|  1|    4|     1|               1.0|               0.0| 2.718281828459045|\n",
      "|  1|    3|     1|               1.0|               0.0| 2.718281828459045|\n",
      "|  2|   24|     4|1.4142135623730951|0.6931471805599453|  7.38905609893065|\n",
      "|  2|   55|     4|1.4142135623730951|0.6931471805599453|  7.38905609893065|\n",
      "|  3|   71|     9|1.7320508075688772|1.0986122886681098|20.085536923187668|\n",
      "|  3|   64|     9|1.7320508075688772|1.0986122886681098|20.085536923187668|\n",
      "|  4|   44|    16|               2.0|1.3862943611198906|54.598150033144236|\n",
      "|  4|   33|    16|               2.0|1.3862943611198906|54.598150033144236|\n",
      "|  5|   11|    25|  2.23606797749979|1.6094379124341003| 148.4131591025766|\n",
      "|  5|   17|    25|  2.23606797749979|1.6094379124341003| 148.4131591025766|\n",
      "|  6|   14|    36| 2.449489742783178| 1.791759469228055| 403.4287934927351|\n",
      "|  6|   16|    36| 2.449489742783178| 1.791759469228055| 403.4287934927351|\n",
      "|  7|   11|    49|2.6457513110645907|1.9459101490553132|1096.6331584284585|\n",
      "|  7|   20|    49|2.6457513110645907|1.9459101490553132|1096.6331584284585|\n",
      "|  8|   17|    64|2.8284271247461903|2.0794415416798357|2980.9579870417283|\n",
      "|  8|   13|    64|2.8284271247461903|2.0794415416798357|2980.9579870417283|\n",
      "|  9|   18|    81|               3.0|2.1972245773362196| 8103.083927575384|\n",
      "|  9|   19|    81|               3.0|2.1972245773362196| 8103.083927575384|\n",
      "| 10|   13|   100|3.1622776601683795| 2.302585092994046|22026.465794806718|\n",
      "| 10|   13|   100|3.1622776601683795| 2.302585092994046|22026.465794806718|\n",
      "+---+-----+------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+-----+------+------------------+------------------+--------------------+\n",
      "|Age|Death|square|              sqrt|                ln|                 exp|\n",
      "+---+-----+------+------------------+------------------+--------------------+\n",
      "|  1|   18|     1|               1.0|               0.0|   2.718281828459045|\n",
      "|  2|   39|     4|1.4142135623730951|0.6931471805599453|    7.38905609893065|\n",
      "|  3|   73|     9|1.7320508075688772|1.0986122886681098|  20.085536923187668|\n",
      "|  4|   99|    16|               2.0|1.3862943611198906|  54.598150033144236|\n",
      "|  5|  143|    25|  2.23606797749979|1.6094379124341003|   148.4131591025766|\n",
      "|  6|  217|    36| 2.449489742783178| 1.791759469228055|   403.4287934927351|\n",
      "|  7|  228|    49|2.6457513110645907|1.9459101490553132|  1096.6331584284585|\n",
      "|  8|  251|    64|2.8284271247461903|2.0794415416798357|  2980.9579870417283|\n",
      "|  9|  246|    81|               3.0|2.1972245773362196|   8103.083927575384|\n",
      "| 10|  267|   100|3.1622776601683795| 2.302585092994046|  22026.465794806718|\n",
      "| 11|  275|   121|   3.3166247903554|2.3978952727983707|   59874.14171519782|\n",
      "| 12|  303|   144|3.4641016151377544|2.4849066497880004|  162754.79141900392|\n",
      "| 13|  321|   169| 3.605551275463989|2.5649493574615367|   442413.3920089205|\n",
      "| 14|  354|   196|3.7416573867739413|2.6390573296152584|  1202604.2841647768|\n",
      "| 15|   21|   225| 3.872983346207417|  2.70805020110221|  3269017.3724721107|\n",
      "| 16|   10|   256|               4.0| 2.772588722239781|   8886110.520507872|\n",
      "| 17|   13|   289| 4.123105625617661| 2.833213344056216|  2.41549527535753E7|\n",
      "| 18|   20|   324| 4.242640687119285|2.8903717578961645| 6.565996913733051E7|\n",
      "| 19|   16|   361| 4.358898943540674|2.9444389791664403|1.7848230096318725E8|\n",
      "| 20|   13|   400|  4.47213595499958| 2.995732273553991| 4.851651954097903E8|\n",
      "+---+-----+------+------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# This cell is for preparing datasets used in regression\n",
    "from math import sqrt, log, exp\n",
    "\n",
    "print(testRDD.take(20))\n",
    "\n",
    "def transformRDD(inputRDD, inCols):\n",
    "    outCols = inCols + ['square', 'sqrt', 'ln', 'exp']\n",
    "    trRDD = inputRDD.filter(lambda x: x[0])\\\n",
    "                    .map(lambda x: [x[0], x[1]] if x[1] else [x[0], 0])\\\n",
    "                    .map(lambda x: [x[0], x[1], x[0] ** 2, sqrt(x[0]), log(x[0]), exp(x[0])])\n",
    "    outDF = sqlContext.createDataFrame(trRDD, outCols)\n",
    "    return outDF\n",
    "\n",
    "testDF = transformRDD(testRDD, testCols)\n",
    "trainDF = transformRDD(trainRDD, trainCols)\n",
    "#trl = ['Age', 'Death', (math.log(sample_test.Death)).alias('ln'), (sample_test.Death ** 0.5).alias('Death_sqrt')]\n",
    "#print(\"attributes of the column are: {}\".format(dir(sample_test.Death)))\n",
    "#trDF = sample_test.select('Age', 'Death', (math.log(sample_test.Death)).alias('ln'), (sample_test.Death ** 0.5).alias('Death_sqrt'))\n",
    "testDF.show()\n",
    "trainDF.show()\n",
    "print(trainRDD.filter(lambda x: not x[0]).filter(lambda x: x[0]).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.regression.LinearRegression'>\n",
      "<class 'list'>\n",
      "[0.156123079079,-10.1427841846] [-0.794247528825,0.00645793136974] [-0.243627194676,9.65907404226e-59]\n",
      "41.81960192255575 30.255080463237025 22.564869554122772\n",
      "Mean Squared Error: 13.31, 12.99, 13.62\n",
      "\n",
      "+---+-----+------+------------------+------------------+------------------+--------------------+------------------+\n",
      "|Age|Death|square|              sqrt|                ln|               exp|            features|         Predicted|\n",
      "+---+-----+------+------------------+------------------+------------------+--------------------+------------------+\n",
      "|  1|    4|     1|               1.0|               0.0| 2.718281828459045|           [1.0,0.0]| 41.97572500163444|\n",
      "|  1|    3|     1|               1.0|               0.0| 2.718281828459045|           [1.0,0.0]| 41.97572500163444|\n",
      "|  2|   24|     4|1.4142135623730951|0.6931471805599453|  7.38905609893065|[2.0,0.6931471805...| 35.10140582009498|\n",
      "|  2|   55|     4|1.4142135623730951|0.6931471805599453|  7.38905609893065|[2.0,0.6931471805...| 35.10140582009498|\n",
      "|  3|   71|     9|1.7320508075688772|1.0986122886681098|20.085536923187668|[3.0,1.0986122886...|31.144983813226787|\n",
      "|  3|   64|     9|1.7320508075688772|1.0986122886681098|20.085536923187668|[3.0,1.0986122886...|31.144983813226787|\n",
      "|  4|   44|    16|               2.0|1.3862943611198906|54.598150033144236|[4.0,1.3862943611...|28.383209717634223|\n",
      "|  4|   33|    16|               2.0|1.3862943611198906|54.598150033144236|[4.0,1.3862943611...|28.383209717634223|\n",
      "|  5|   11|    25|  2.23606797749979|1.6094379124341003| 148.4131591025766|[5.0,1.6094379124...|26.276035913536518|\n",
      "|  5|   17|    25|  2.23606797749979|1.6094379124341003| 148.4131591025766|[5.0,1.6094379124...|26.276035913536518|\n",
      "|  6|   14|    36| 2.449489742783178| 1.791759469228055| 403.4287934927351|[6.0,1.7917594692...|24.582910789844718|\n",
      "|  6|   16|    36| 2.449489742783178| 1.791759469228055| 403.4287934927351|[6.0,1.7917594692...|24.582910789844718|\n",
      "|  7|   11|    49|2.6457513110645907|1.9459101490553132|1096.6331584284585|[7.0,1.9459101490...|23.175516791518447|\n",
      "|  7|   20|    49|2.6457513110645907|1.9459101490553132|1096.6331584284585|[7.0,1.9459101490...|23.175516791518447|\n",
      "|  8|   17|    64|2.8284271247461903|2.0794415416798357|2980.9579870417283|[8.0,2.0794415416...|21.977259773330843|\n",
      "|  8|   13|    64|2.8284271247461903|2.0794415416798357|2980.9579870417283|[8.0,2.0794415416...|21.977259773330843|\n",
      "|  9|   18|    81|               3.0|2.1972245773362196| 8103.083927575384|[9.0,2.1972245773...|20.938734941133895|\n",
      "|  9|   19|    81|               3.0|2.1972245773362196| 8103.083927575384|[9.0,2.1972245773...|20.938734941133895|\n",
      "| 10|   13|   100|3.1622776601683795| 2.302585092994046|22026.465794806718|[10.0,2.302585092...| 20.02620904831182|\n",
      "| 10|   13|   100|3.1622776601683795| 2.302585092994046|22026.465794806718|[10.0,2.302585092...| 20.02620904831182|\n",
      "+---+-----+------+------------------+------------------+------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+---+-----+------+------------------+------------------+------------------+------------+------------------+\n",
      "|Age|Death|square|              sqrt|                ln|               exp|    features|         Predicted|\n",
      "+---+-----+------+------------------+------------------+------------------+------------+------------------+\n",
      "|  1|    4|     1|               1.0|               0.0| 2.718281828459045|   [1.0,1.0]| 29.46729086578156|\n",
      "|  1|    3|     1|               1.0|               0.0| 2.718281828459045|   [1.0,1.0]| 29.46729086578156|\n",
      "|  2|   24|     4|1.4142135623730951|0.6931471805599453|  7.38905609893065|   [2.0,4.0]| 28.69241713106557|\n",
      "|  2|   55|     4|1.4142135623730951|0.6931471805599453|  7.38905609893065|   [2.0,4.0]| 28.69241713106557|\n",
      "|  3|   71|     9|1.7320508075688772|1.0986122886681098|20.085536923187668|   [3.0,9.0]| 27.93045925908906|\n",
      "|  3|   64|     9|1.7320508075688772|1.0986122886681098|20.085536923187668|   [3.0,9.0]| 27.93045925908906|\n",
      "|  4|   44|    16|               2.0|1.3862943611198906|54.598150033144236|  [4.0,16.0]|27.181417249852018|\n",
      "|  4|   33|    16|               2.0|1.3862943611198906|54.598150033144236|  [4.0,16.0]|27.181417249852018|\n",
      "|  5|   11|    25|  2.23606797749979|1.6094379124341003| 148.4131591025766|  [5.0,25.0]|26.445291103354457|\n",
      "|  5|   17|    25|  2.23606797749979|1.6094379124341003| 148.4131591025766|  [5.0,25.0]|26.445291103354457|\n",
      "|  6|   14|    36| 2.449489742783178| 1.791759469228055| 403.4287934927351|  [6.0,36.0]|25.722080819596368|\n",
      "|  6|   16|    36| 2.449489742783178| 1.791759469228055| 403.4287934927351|  [6.0,36.0]|25.722080819596368|\n",
      "|  7|   11|    49|2.6457513110645907|1.9459101490553132|1096.6331584284585|  [7.0,49.0]|25.011786398577758|\n",
      "|  7|   20|    49|2.6457513110645907|1.9459101490553132|1096.6331584284585|  [7.0,49.0]|25.011786398577758|\n",
      "|  8|   17|    64|2.8284271247461903|2.0794415416798357|2980.9579870417283|  [8.0,64.0]| 24.31440784029862|\n",
      "|  8|   13|    64|2.8284271247461903|2.0794415416798357|2980.9579870417283|  [8.0,64.0]| 24.31440784029862|\n",
      "|  9|   18|    81|               3.0|2.1972245773362196| 8103.083927575384|  [9.0,81.0]| 23.62994514475896|\n",
      "|  9|   19|    81|               3.0|2.1972245773362196| 8103.083927575384|  [9.0,81.0]| 23.62994514475896|\n",
      "| 10|   13|   100|3.1622776601683795| 2.302585092994046|22026.465794806718|[10.0,100.0]|22.958398311958774|\n",
      "| 10|   13|   100|3.1622776601683795| 2.302585092994046|22026.465794806718|[10.0,100.0]|22.958398311958774|\n",
      "+---+-----+------+------------------+------------------+------------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+---+-----+------+------------------+------------------+------------------+--------------------+------------------+\n",
      "|Age|Death|square|              sqrt|                ln|               exp|            features|         Predicted|\n",
      "+---+-----+------+------------------+------------------+------------------+--------------------+------------------+\n",
      "|  1|    4|     1|               1.0|               0.0| 2.718281828459045|[1.0,2.7182818284...|22.321242359446686|\n",
      "|  1|    3|     1|               1.0|               0.0| 2.718281828459045|[1.0,2.7182818284...|22.321242359446686|\n",
      "|  2|   24|     4|1.4142135623730951|0.6931471805599453|  7.38905609893065|[2.0,7.3890560989...|  22.0776151647706|\n",
      "|  2|   55|     4|1.4142135623730951|0.6931471805599453|  7.38905609893065|[2.0,7.3890560989...|  22.0776151647706|\n",
      "|  3|   71|     9|1.7320508075688772|1.0986122886681098|20.085536923187668|[3.0,20.085536923...|21.833987970094512|\n",
      "|  3|   64|     9|1.7320508075688772|1.0986122886681098|20.085536923187668|[3.0,20.085536923...|21.833987970094512|\n",
      "|  4|   44|    16|               2.0|1.3862943611198906|54.598150033144236|[4.0,54.598150033...|21.590360775418425|\n",
      "|  4|   33|    16|               2.0|1.3862943611198906|54.598150033144236|[4.0,54.598150033...|21.590360775418425|\n",
      "|  5|   11|    25|  2.23606797749979|1.6094379124341003| 148.4131591025766|[5.0,148.41315910...|21.346733580742338|\n",
      "|  5|   17|    25|  2.23606797749979|1.6094379124341003| 148.4131591025766|[5.0,148.41315910...|21.346733580742338|\n",
      "|  6|   14|    36| 2.449489742783178| 1.791759469228055| 403.4287934927351|[6.0,403.42879349...| 21.10310638606625|\n",
      "|  6|   16|    36| 2.449489742783178| 1.791759469228055| 403.4287934927351|[6.0,403.42879349...| 21.10310638606625|\n",
      "|  7|   11|    49|2.6457513110645907|1.9459101490553132|1096.6331584284585|[7.0,1096.6331584...|20.859479191390164|\n",
      "|  7|   20|    49|2.6457513110645907|1.9459101490553132|1096.6331584284585|[7.0,1096.6331584...|20.859479191390164|\n",
      "|  8|   17|    64|2.8284271247461903|2.0794415416798357|2980.9579870417283|[8.0,2980.9579870...| 20.61585199671408|\n",
      "|  8|   13|    64|2.8284271247461903|2.0794415416798357|2980.9579870417283|[8.0,2980.9579870...| 20.61585199671408|\n",
      "|  9|   18|    81|               3.0|2.1972245773362196| 8103.083927575384|[9.0,8103.0839275...|20.372224802037994|\n",
      "|  9|   19|    81|               3.0|2.1972245773362196| 8103.083927575384|[9.0,8103.0839275...|20.372224802037994|\n",
      "| 10|   13|   100|3.1622776601683795| 2.302585092994046|22026.465794806718|[10.0,22026.46579...|20.128597607361908|\n",
      "| 10|   13|   100|3.1622776601683795| 2.302585092994046|22026.465794806718|[10.0,22026.46579...|20.128597607361908|\n",
      "+---+-----+------+------------------+------------------+------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, LinearRegression, LinearRegressionModel, RandomForestRegressor\n",
    "\n",
    "def Do_Machine_Learning(trainingSet, testSet, xCols, yValues, regressor = LinearRegression(),\n",
    "                         paramGrid = [], evalMetric = \"rmse\", seed = None):\n",
    "    \"\"\"\n",
    "    Return (<object> model, <float> error_estimate, <dataframe> result)\n",
    "    \n",
    "    trainingSet: dataframe, used to train model\n",
    "    testSet: dataframe, used to feed the model and get the result(and error estimate)\n",
    "    xCols: list of strings, names of columns which used as inputs\n",
    "    yValues: string, name of column that contains dependent values \n",
    "    regressor: Regression object, by default = LinearRegression()\n",
    "    paramGrid: list built byParamGridBuilder, by default = empty list\n",
    "    evalMetric: string, name of matrix used for evaluation, by default = \"rmse\"\n",
    "    seed: int or None, seed for random number generator, if == None will use random numbers\n",
    "    \n",
    "    !!! seed is useless at that time !!!\n",
    "    \"\"\"\n",
    "    # push estimator into pipeline\n",
    "    vec = VectorAssembler(inputCols = xCols, outputCol = \"features\")\n",
    "    regPipeline = Pipeline()\n",
    "    regPipeline.setStages([vec, regressor])   \n",
    "    # build evaluator\n",
    "    regEval = RegressionEvaluator(predictionCol = \"Predicted\", labelCol = yValues, metricName = evalMetric)\n",
    "    # combine estimator and evaluator to a cross validator\n",
    "    crossval = CrossValidator(estimator = regPipeline, evaluator = regEval, numFolds = 3)\n",
    "    # set parameters grid\n",
    "    crossval.setEstimatorParamMaps(paramGrid)\n",
    "    # trainning\n",
    "    regModel = crossval.fit(trainingSet).bestModel\n",
    "    # predicting\n",
    "    predictions = regModel.transform(testSet)\n",
    "    # get evaluating result\n",
    "    evaluation = regEval.evaluate(predictions)\n",
    "    \n",
    "    return regModel, evaluation, predictions\n",
    "\n",
    "# build regressor\n",
    "lr1 = LinearRegression()\n",
    "lr1.setPredictionCol(\"Predicted\")\\\n",
    "   .setLabelCol(\"Death\")\n",
    "\n",
    "# build parameter grid\n",
    "regParam = [x / 100.0 for x in range(1, 10)]\n",
    "pg1 = (ParamGridBuilder()\n",
    "             .addGrid(lr1.regParam, regParam)\n",
    "             .build())\n",
    "\n",
    "print(type(lr1))\n",
    "print(type(pg1))\n",
    "\n",
    "# run ML\n",
    "model1, result1, predictionDF1 = Do_Machine_Learning(trainDF, testDF, [\"Age\", \"ln\"], \"Death\", lr1, pg1)\n",
    "model2, result2, predictionDF2 = Do_Machine_Learning(trainDF, testDF, [\"Age\", \"square\"], \"Death\", lr1, pg1)\n",
    "model3, result3, predictionDF3 = Do_Machine_Learning(trainDF, testDF, [\"Age\", \"exp\"], \"Death\", lr1, pg1)\n",
    "\n",
    "\n",
    "# print attributions of model\n",
    "\"\"\"\n",
    "print(\"attributes of the model are: {}\".format(dir(model)))\n",
    "print(\"method list: {}\".format([method for method in dir(model) if callable(getattr(model, method))]))\n",
    "print(model.stages)\n",
    "\"\"\"\n",
    "\n",
    "# Print coefficients and intercept\n",
    "weights1 = model1.stages[1].coefficients\n",
    "ic1 = model1.stages[1].intercept\n",
    "weights2 = model2.stages[1].coefficients\n",
    "ic2 = model2.stages[1].intercept\n",
    "weights3 = model3.stages[1].coefficients\n",
    "ic3 = model3.stages[1].intercept\n",
    "print(weights1, weights2, weights3)\n",
    "print(ic1, ic2, ic3)\n",
    "#print(list(zip([\"Age\"], weights1)))\n",
    "#print(model.stages[1].intercept)\n",
    "\n",
    "# print error and result\n",
    "print(\"Mean Squared Error: {0:2.2f}, {1:2.2f}, {2:2.2f}\\n\".format(result1, result2, result3))\n",
    "predictionDF1.show()\n",
    "predictionDF2.show()\n",
    "predictionDF3.show()\n",
    "\n",
    "# print the model\n",
    "#print(model.stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.regression.LinearRegression'>\n",
      "[{Param(parent='LinearRegression_4dc5b27433f5df6b3c15', name='regParam', doc='regularization parameter (>= 0).'): 0.01}, {Param(parent='LinearRegression_4dc5b27433f5df6b3c15', name='regParam', doc='regularization parameter (>= 0).'): 0.02}, {Param(parent='LinearRegression_4dc5b27433f5df6b3c15', name='regParam', doc='regularization parameter (>= 0).'): 0.03}, {Param(parent='LinearRegression_4dc5b27433f5df6b3c15', name='regParam', doc='regularization parameter (>= 0).'): 0.04}, {Param(parent='LinearRegression_4dc5b27433f5df6b3c15', name='regParam', doc='regularization parameter (>= 0).'): 0.05}, {Param(parent='LinearRegression_4dc5b27433f5df6b3c15', name='regParam', doc='regularization parameter (>= 0).'): 0.06}, {Param(parent='LinearRegression_4dc5b27433f5df6b3c15', name='regParam', doc='regularization parameter (>= 0).'): 0.07}, {Param(parent='LinearRegression_4dc5b27433f5df6b3c15', name='regParam', doc='regularization parameter (>= 0).'): 0.08}, {Param(parent='LinearRegression_4dc5b27433f5df6b3c15', name='regParam', doc='regularization parameter (>= 0).'): 0.09}]\n"
     ]
    }
   ],
   "source": [
    "print(type(lr1))\n",
    "print(pg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python with Pixiedust (Spark 2.2)",
   "language": "python",
   "name": "pythonwithpixiedustspark22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
